{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/data/miniconda3/envs/env_clone-3.9.16/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/miniconda3/envs/env_clone-3.9.16/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.\n",
      "  warnings.warn(f\"Warning: The {path} owner does not match the current user.\")\n",
      "/data/miniconda3/envs/env_clone-3.9.16/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC2.alpha003/x86_64-linux/ascend_toolkit_install.info owner does not match the current user.\n",
      "  warnings.warn(f\"Warning: The {path} owner does not match the current user.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import faiss\n",
    "import numpy as np\n",
    "from simcse import SimCSE\n",
    "from shared.prompt import instance\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from KNN_RE import get_train_example, find_knn_example, find_lmknn_example\n",
    "from load_model import get_emb,Args\n",
    "from label_explain import SemEval_label2exp\n",
    "sys.path.append(\"/group/40064/johnbli/Code/UGC\")\n",
    "from examples.gpt_http import retry_request_openai_summary\n",
    "\n",
    "params = {\n",
    "    # 任务信息\n",
    "    \"env\": \"production\",  # 必填，（取值范围：production、development）\n",
    "    \"task_name\": \"逃出精神病院\",\n",
    "    \"task_type\": \"文本生成任务\",  # 必填，任务类型（取值范围：qa任务、文本生成任务、整段对话生产、多轮对话、其他）\n",
    "    \"task_description\": \"\",\n",
    "    \"business_id\":\"134\"\n",
    "}\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/group/40064/johnbli/Code/GPT-RE/dataset/semeval_gpt/test.json\"\n",
    "\n",
    "test_data = []\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        test_data.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sentences': [['the',\n",
       "   'most',\n",
       "   'common',\n",
       "   'audits',\n",
       "   'were',\n",
       "   'about',\n",
       "   'waste',\n",
       "   'and',\n",
       "   'recycling',\n",
       "   '.']],\n",
       " 'ner': [[[3, 3, 'sub'], [6, 6, 'obj']]],\n",
       " 'predicted_ner': [[[3, 3, 'sub'], [6, 6, 'obj']]],\n",
       " 'relations': [[[3, 3, 6, 6, 'Message-Topic']]],\n",
       " 'doc_key': '0oftrain'}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_idtoprompt = {0:\"NONE\",1:\"CAUSE AND EFFECT\", 2:\"COMPONENT AND WHOLE\", 3:\"ENTITY AND DESTINATION\",4:\"ENTITY AND ORIGIN\",5:\"PRODUCT AND PRODUCER\",6:\"MEMBER AND COLLECTION\",7:\"MESSAGE AND TOPIC\",8:\"CONTENT AND CONTAINER\",9:\"INSTRUMENT AND AGENCY\"}\n",
    "semeval_reltoid = {\"Other\":0,\"Cause-Effect\":1, \"Component-Whole\":2, \"Entity-Destination\":3, \"Entity-Origin\":4, \"Product-Producer\": 5, \"Member-Collection\":6, \"Message-Topic\": 7, \"Content-Container\":8, \"Instrument-Agency\":9}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_PROMPT = '''\n",
    "You are a relation extraction expert specializing in the field of relation extraction, responsible for identifying and extracting the relationship between entities from text to support applications such as information retrieval, knowledge graph construction and data analysis. The following is the entity information:\n",
    "\n",
    "Entity 1: {ent1}\n",
    "\n",
    "Entity 2: {ent2}\n",
    "\n",
    "The categories of relations include the following: {rels}\n",
    "\n",
    "Your task is to process the text given below to accurately identify and extract the relationship between entity 1 and entity 2 in the text from the list of relation categories.\n",
    "\n",
    "**Requirements**\n",
    "You must accurately identify the type of relationship between entities in the sentence.\n",
    "The response should not contain any information other than the entity category.\n",
    "\n",
    "**Examples**\n",
    "{examples}\n",
    "\n",
    "Please judge the relationship between the following sentences based on the above examples:\n",
    "**Input**\n",
    "{sen}\n",
    "\n",
    "Output format:\n",
    "Relation: XXX\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLaMA_PROMPT = '''\n",
    "You are a system prompt generation expert specializing in the field of relation extraction, responsible for identifying and extracting the relationship between entities from text to support applications such as information retrieval, knowledge graph construction and data analysis. The following is the entity information:\n",
    "\n",
    "Entity 1: {ent1}\n",
    "\n",
    "Entity 2: {ent2}\n",
    "\n",
    "The categories of relations include the following: {rels}\n",
    "\n",
    "Your task is to process the text given below to accurately identify and extract the relationship between entity 1 and entity 2 in the text from the list of relation categories.\n",
    "\n",
    "**Examples**\n",
    "{examples}\n",
    "\n",
    "**Requirements**\n",
    "You must accurately identify the type of relationship between entities in the sentence\n",
    "The response should not contain any information other than the entity category\n",
    "\n",
    "Please judge the relationship between the following sentences based on the above examples:\n",
    "**Input**\n",
    "{sen}\n",
    "\n",
    "Output format:\n",
    "Relation: XXX\n",
    "\n",
    "'''"
   ]
  },
  {
   "source": [
    "**GPT**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_first_dia(ent1, ent2, rels, examples, sen):\n",
    "    prompt = GPT_PROMPT.format(ent1 = ent1,ent2 = ent2,rels=rels,examples=examples, sen = sentence)\n",
    "    # print(prompt)\n",
    "    res = retry_request_openai_summary(prompt, **params)[0]\n",
    "    res_lst = [item for item in res.split(\"\\n\") if item]\n",
    "    return res_lst\n",
    "\n",
    "def gen_GPT_first_chat(ent1, ent2, rels,examples, sen):\n",
    "    res_lst = gen_first_dia(ent1, ent2, rels,examples, sen)\n",
    "    return res_lst"
   ]
  },
  {
   "source": [
    "**Qwen2.5-70B**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chat(messages, model_namespace='Production', model_service_name='DataGPTv1ServerVllm'):\n",
    "\n",
    "    req = {\n",
    "        \"messages\": messages,\n",
    "        \"model_version\": \"default\",\n",
    "        \"task_id\": time.time(),\n",
    "    }\n",
    "    # print(req)\n",
    "\n",
    "    # 拼接请求url\n",
    "    url = f'http://11.135.133.241:13023/v1/chat'\n",
    "\n",
    "    \n",
    "    # 发起请求\n",
    "    ret = requests.request(\"POST\", url, headers={\"Content-Type\": \"application/json\"},\n",
    "                           data=json.dumps(req))\n",
    "    # print(type(ret))\n",
    "    # print(ret.status_code)\n",
    "\n",
    "    result = json.loads(ret.text)\n",
    "\n",
    "    return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def predict_chat_prompt(prompt, model_namespace='Production', model_service_name='DataGPTv1ServerVllm'):\n",
    "\n",
    "    req = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model_version\": \"default\",\n",
    "        \"task_id\": time.time(),\n",
    "    }\n",
    "    # print(req)\n",
    "\n",
    "    # 拼接请求url\n",
    "    url = f'http://11.135.133.241:13023/v1/chat'\n",
    "\n",
    "    \n",
    "    # 发起请求\n",
    "    ret = requests.request(\"POST\", url, headers={\"Content-Type\": \"application/json\"},\n",
    "                           data=json.dumps(req))\n",
    "    # print(type(ret))\n",
    "    # print(ret.status_code)\n",
    "\n",
    "    result = json.loads(ret.text)\n",
    "\n",
    "    return result[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Qwen_res(ent1, ent2, rels, sen):\n",
    "    # print(chai)\n",
    "    prompt = GPT_PROMPT.format(ent1 = ent1,ent2 = ent2,rels=rels, sen = sen)\n",
    "    messages = [{'role': 'system', 'content': 'You are an AI assistant '}, {'role': 'user', 'content': prompt}]\n",
    "    res_lst = predict_chat(messages)\n",
    "    return [res_lst]"
   ]
  },
  {
   "source": [
    "**Qwen2.5-7B**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = '/group/40064/heyfonli/model_hub/Qwen2.5-7B-Instruct/'\n",
    "# model_name_or_path = \"/group/40064/johnbli/LLM-based-models/LLama3/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  8.49s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='npu:1', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Qwen_res_7b(ent1, ent2, rels, examples, sentence):\n",
    "    prompt = GPT_PROMPT.format(ent1 = ent1,ent2 = ent2,rels=rels,examples=examples, sen = sentence)\n",
    "    messages = [{'role': 'system', 'content': 'You are an AI assistant '}, {'role': 'user', 'content': prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    # print(text)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to('npu:1')\n",
    "    generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_p = 0,\n",
    "            top_k = 1\n",
    "        )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return [response]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SUP = '''\n",
    "Sample {num}:\n",
    "    Entity 1: {ent1}\n",
    "    Entity 2: {ent2}\n",
    "    Input:{sen}\n",
    "    Output:Relation: {label}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_instance = True\n",
    "use_knn = True\n",
    "use_pipline = False\n",
    "k_sample = 5\n",
    "\n",
    "example_dict = get_train_example(\"/group/40064/johnbli/Code/GPT-RE/dataset/semeval_gpt/train.json\", semeval_reltoid, 0)\n",
    "train_list = [x for y in example_dict.values() for x in y]\n",
    "# train_list = [x for x in train_list if semeval_reltoid[x[\"relations\"][0][0][4]] != 0]\n",
    "train_sentences = [instance(x).sentence for x in train_list]\n",
    "train_reference = [instance(x).reference for x in train_list]\n",
    "train_dict = {instance(x).reference:x for x in train_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/data/miniconda3/envs/env_clone-3.9.16/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "01/04/2025 21:01:50 - INFO - simcse.tool -   Encoding embeddings for sentences...\n",
      "100%|██████████| 102/102 [02:14<00:00,  1.32s/it]\n",
      "01/04/2025 21:04:04 - INFO - simcse.tool -   Building index...\n",
      "01/04/2025 21:04:04 - INFO - simcse.tool -   Use CPU-version faiss\n",
      "01/04/2025 21:04:04 - INFO - simcse.tool -   Finished\n"
     ]
    }
   ],
   "source": [
    "if use_knn:\n",
    "    knn_model = SimCSE(\"/group/40064/johnbli/bert-based-models/sup-simcse-roberta-large\")\n",
    "    knn_model.build_index(train_reference, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if use_pipline:\n",
    "\n",
    "    # res = faiss.StandardGpuResources()  # 不再需要初始化GPU资\n",
    "    # gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)  # 不需要转移到GPU\n",
    "    index_flat = faiss.IndexFlatL2(768)  # 创建一个使用L2距离的平面索引\n",
    "    cpu_index_flat = index_flat  # 直接使用CPU索引\n",
    "    args =Args()\n",
    "    dataset = 'SemEval'\n",
    "    retriever_plm = \"/group/40064/johnbli/bert-based-models/bert_base_uncased\"\n",
    "    device = torch.device(\"npu:7\")\n",
    "    train_samples = get_emb(args, train_list, dataset, retriever_plm, k_sample, device) #使用全部数据集\n",
    "    # train_samples = get_emb(args, \"\", dataset, retriever_plm, k_sample, device) #使用部分数据集\n",
    "\n",
    "    embed_list = np.array(train_samples)\n",
    "    cpu_index_flat.add(embed_list)  # 使用CPU索引添加特征向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(item):\n",
    "    sentences_list = item[\"sentences\"][0]\n",
    "    ent1 = \" \".join(sentences_list[item[\"ner\"][0][0][0]:item[\"ner\"][0][0][1]+1])\n",
    "    ent2 = \" \".join(sentences_list[item[\"ner\"][0][1][0]:item[\"ner\"][0][1][1]+1])\n",
    "    sentence = \" \".join(sentences_list)\n",
    "    label = item[\"relations\"][0][0][-1]\n",
    "    return ent1,ent2,sentence,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 43/43 [00:51<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "if use_knn:\n",
    "    knn_select = find_knn_example(knn_model, test_data, train_dict, k_sample, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_pipline:\n",
    "    test_samples = get_emb(args, test_data, dataset, retriever_plm, k_sample, device)\n",
    "    D, I = cpu_index_flat.search(test_samples, k_sample)\n",
    "    lm_select = []\n",
    "    for j in range(I.shape[0]):\n",
    "        j_select = [train_dict[train_sentences[i]] for i in I[j,:k_sample]]\n",
    "        lm_select.append(j_select)\n",
    "    # lm_select = find_lmknn_example(cpu_index_flat, test_data, train_dict, train_sentences, k_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/2717 [00:00<?, ?it/s]/data/miniconda3/envs/env_clone-3.9.16/lib/python3.9/site-packages/transformers/generation/logits_process.py:464: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at torch_npu/csrc/aten/common/TensorFactories.cpp:74.)\n",
      "  sorted_indices_to_remove[..., -self.min_tokens_to_keep :] = 0\n",
      "[W VariableFallbackKernel.cpp:51] Warning: CAUTION: The operator 'aten::isin.Tensor_Tensor_out' is not currently supported on the NPU backend and will fall back to run on the CPU. This may have performance implications. (function npu_cpu_fallback)\n",
      "100%|██████████| 2717/2717 [17:28<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "Labels = []\n",
    "predict = []\n",
    "num = 0\n",
    "ids = 0\n",
    "for test_item in tqdm(test_data):\n",
    "\n",
    "    sentences_list = test_item[\"sentences\"][0]\n",
    "    ent1 = \" \".join(sentences_list[test_item[\"ner\"][0][0][0]:test_item[\"ner\"][0][0][1]+1])\n",
    "    ent2 = \" \".join(sentences_list[test_item[\"ner\"][0][1][0]:test_item[\"ner\"][0][1][1]+1])\n",
    "    sentence = \" \".join(sentences_list)\n",
    "    rels = \"\"\n",
    "    rels_lst = {}\n",
    "    for k,v in semeval_idtoprompt.items():\n",
    "        rels = rels + v + '; '\n",
    "        rels_lst[v] = k\n",
    "    # 更详细的关系描述\n",
    "    # for k,v in SemEval_label2exp.items():  \n",
    "    #     rels = rels + k + \":\" + v + '\\n'\n",
    "    label = semeval_reltoid[test_item[\"relations\"][0][0][-1]]\n",
    "    Labels.append(label)\n",
    "    # 添加instance\n",
    "    example = \"\"\n",
    "    if use_instance:\n",
    "        if use_knn:\n",
    "            selected_items = knn_select[ids]\n",
    "            ids += 1\n",
    "            # print(selected_items)\n",
    "        elif use_pipline:\n",
    "            selected_items = lm_select[ids]\n",
    "            ids += 1\n",
    "        else:\n",
    "            selected_items = random.sample(train_list, k_sample)\n",
    "        for i,item in enumerate(selected_items):\n",
    "            train_ent1,train_ent2,train_sentence,train_label = get_example(item)\n",
    "            train_label = semeval_idtoprompt[semeval_reltoid[train_label]]\n",
    "            example = example + TRAIN_SUP.format(num = i, ent1 = train_ent1,ent2 = train_ent2,label=train_label, sen = train_sentence)\n",
    "\n",
    "    # res_lst = get_Qwen_res(ent1, ent2, rels, sentence)    #Qwen2.5-70b\n",
    "    # res_lst = gen_GPT_first_chat(ent1, ent2, rels, example, sentence)     #GPT-70b\n",
    "    res_lst = get_Qwen_res_7b(ent1, ent2, rels,example, sentence)        #Qwen2.5-7b\n",
    "    \n",
    "    res = res_lst[0].replace(\"Relation: \",\"\") # 无理由\n",
    "    # res = res_lst[0].split('\\n')[-1].replace(\"Relation: \",\"\")  #有理由\n",
    "    # print(res)\n",
    "\n",
    "    if res not in rels_lst:\n",
    "        num += 1\n",
    "        predict.append(0 if label!=0 else 1)\n",
    "    else:\n",
    "        predict.append(rels_lst[res])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "micro_f1 Score: 0.6013986013986014\nmacro_f1 Score: 0.5975918708678143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 计算 F1 值\n",
    "macro_f1 = f1_score(Labels, predict, average='macro')  # 'weighted' 考虑各类标签的不平衡问题\n",
    "micro_f1 = f1_score(Labels, predict, average='micro')  # 'weighted' 考虑各类标签的不平衡问题\n",
    "\n",
    "print(\"micro_f1 Score:\", micro_f1)\n",
    "print(\"macro_f1 Score:\", macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ENTITY AND DESTINATION'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.16 64-bit ('env_clone-3.9.16': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7ae9c97f290ede2c3f05d2d7cfbff41b9b48cc67ac3d685f1cae8159fc32dd86"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}